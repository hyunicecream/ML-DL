{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras(BostonHousing_Data).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOLom9lOFDs1g84zNWw5XKD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunicecream/ML-DL/blob/main/Keras(BostonHousing%20Data).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cy2XCUpt8yMx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_boston\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.regularizers import l1, l2, l1_l2 #overfitting 방지 방법 2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "boston = load_boston()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wm73_T6F9H_F",
        "outputId": "ecd74d5b-4875-4c6f-a43e-df8e98d0b596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(boston.data, columns=[boston.feature_names])\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "FCAWZhwJ9Ngn",
        "outputId": "585b4452-d60b-4bb8-8ce4-37e792e30123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>501</th>\n",
              "      <td>0.06263</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.593</td>\n",
              "      <td>69.1</td>\n",
              "      <td>2.4786</td>\n",
              "      <td>1.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>391.99</td>\n",
              "      <td>9.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>502</th>\n",
              "      <td>0.04527</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.120</td>\n",
              "      <td>76.7</td>\n",
              "      <td>2.2875</td>\n",
              "      <td>1.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503</th>\n",
              "      <td>0.06076</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.976</td>\n",
              "      <td>91.0</td>\n",
              "      <td>2.1675</td>\n",
              "      <td>1.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>504</th>\n",
              "      <td>0.10959</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.794</td>\n",
              "      <td>89.3</td>\n",
              "      <td>2.3889</td>\n",
              "      <td>1.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>393.45</td>\n",
              "      <td>6.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>505</th>\n",
              "      <td>0.04741</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.030</td>\n",
              "      <td>80.8</td>\n",
              "      <td>2.5050</td>\n",
              "      <td>1.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>7.88</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>506 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        CRIM    ZN  INDUS CHAS    NOX  ...  RAD    TAX PTRATIO       B LSTAT\n",
              "0    0.00632  18.0   2.31  0.0  0.538  ...  1.0  296.0    15.3  396.90  4.98\n",
              "1    0.02731   0.0   7.07  0.0  0.469  ...  2.0  242.0    17.8  396.90  9.14\n",
              "2    0.02729   0.0   7.07  0.0  0.469  ...  2.0  242.0    17.8  392.83  4.03\n",
              "3    0.03237   0.0   2.18  0.0  0.458  ...  3.0  222.0    18.7  394.63  2.94\n",
              "4    0.06905   0.0   2.18  0.0  0.458  ...  3.0  222.0    18.7  396.90  5.33\n",
              "..       ...   ...    ...  ...    ...  ...  ...    ...     ...     ...   ...\n",
              "501  0.06263   0.0  11.93  0.0  0.573  ...  1.0  273.0    21.0  391.99  9.67\n",
              "502  0.04527   0.0  11.93  0.0  0.573  ...  1.0  273.0    21.0  396.90  9.08\n",
              "503  0.06076   0.0  11.93  0.0  0.573  ...  1.0  273.0    21.0  396.90  5.64\n",
              "504  0.10959   0.0  11.93  0.0  0.573  ...  1.0  273.0    21.0  393.45  6.48\n",
              "505  0.04741   0.0  11.93  0.0  0.573  ...  1.0  273.0    21.0  396.90  7.88\n",
              "\n",
              "[506 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 표준화하기\n",
        "scaleX = StandardScaler()\n",
        "feature_data = scaleX.fit_transform(boston.data)\n",
        "scaleY = StandardScaler()\n",
        "target_data = scaleY.fit_transform(boston.target.reshape(-1, 1)) # shape 맞춰주기, "
      ],
      "metadata": {
        "id": "c0qr675i9o4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 데이터와 시험 데이터로 분리한다.\n",
        "x_train,x_test,y_train,y_test = train_test_split(feature_data, target_data, test_size=0.2)\n",
        "x_train.shape, y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuJnf7Io-G7F",
        "outputId": "fe30bca8-df70-435b-97fe-e5757f62ac58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((404, 13), (404, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 신경망 생성\n",
        "# OverFitting 방지방법 1 : Dropout 사용\n",
        "# OverFitting 방지방법 2 : l1, l2, l1_l2 사용\n",
        "# OverFitting 방지방법 3 : BatchNormalization(), Activation('relu')\n",
        "\n",
        "x_input = Input(batch_shape = (None, 13))\n",
        "h_layer = Dense(256, activation='relu')(x_input)\n",
        "h_layer = Dropout(rate=0.5)(h_layer) # overfitting 방지1\n",
        "h_layer = Dense(256, activation='relu', kernel_regularizer=l2(0.00))(h_layer) # overfitting 방지2\n",
        "h_layer = Dense(256)(h_layer)\n",
        "h_layer = BatchNormalization()(h_layer)\n",
        "h_layer = Activation('relu')(h_layer)\n",
        "y_output = Dense(1)(h_layer)\n",
        "\n",
        "model = Model(x_input, y_output)\n",
        "# Regression문제는 mse로 정의한다. \n",
        "# model.compile(loss='mse', optimizer=Adam(learning_rate=0.5)) 위에 optimaizer로 불러와서 따로 learning_rate를 설정할 수 있다. \n",
        "# learing_rate가 작으면 과거의 경험 중시(장기 이동평균ex:0.01), 안정적, 더 나은 결과 노력부족/ 크면 과거의 경험 무시, 현재 중시 (단기이동 평균), 변화적응력은 좋지만 안전성이 떨어진다. \n",
        "model.compile(loss='mse', optimizer=Adam(learning_rate=0.5))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tha8mEH-a2Q",
        "outputId": "4928593e-f22c-4329-92d7-fdbd76d7173b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 13)]              0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 256)               3584      \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 256)              1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 256)               0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 136,449\n",
            "Trainable params: 135,937\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit(x_train, y_train, batch_size = 10, epochs=50, validation_data = (x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aSh7nLtAhyv",
        "outputId": "a2177ab7-138b-4e95-d6a7-9d9952622dff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "41/41 [==============================] - 1s 8ms/step - loss: 90.5986 - val_loss: 7739.8525\n",
            "Epoch 2/50\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 2.8109 - val_loss: 537.9417\n",
            "Epoch 3/50\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 1.1793 - val_loss: 32.0729\n",
            "Epoch 4/50\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.9366 - val_loss: 0.8627\n",
            "Epoch 5/50\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.8264 - val_loss: 0.8505\n",
            "Epoch 6/50\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.7192 - val_loss: 0.5369\n",
            "Epoch 7/50\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 0.5894 - val_loss: 0.4479\n",
            "Epoch 8/50\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 0.6221 - val_loss: 0.3286\n",
            "Epoch 9/50\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.6542 - val_loss: 0.7591\n",
            "Epoch 10/50\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 0.5787 - val_loss: 0.9385\n",
            "Epoch 11/50\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 0.5746 - val_loss: 1.5926\n",
            "Epoch 12/50\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.5051 - val_loss: 0.7054\n",
            "Epoch 13/50\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 0.4954 - val_loss: 1.3526\n",
            "Epoch 14/50\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 0.5127 - val_loss: 1.0316\n",
            "Epoch 15/50\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.5507 - val_loss: 1.3641\n",
            "Epoch 16/50\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.5528 - val_loss: 1.5202\n",
            "Epoch 17/50\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 0.4225 - val_loss: 1.6173\n",
            "Epoch 18/50\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.4408 - val_loss: 0.8104\n",
            "Epoch 19/50\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 0.5665 - val_loss: 2.8776\n",
            "Epoch 20/50\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 0.5477 - val_loss: 0.8831\n",
            "Epoch 21/50\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 0.4158 - val_loss: 1.7375\n",
            "Epoch 22/50\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.5227 - val_loss: 0.9323\n",
            "Epoch 23/50\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 0.5852 - val_loss: 0.1582\n",
            "Epoch 24/50\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.3936 - val_loss: 0.8136\n",
            "Epoch 25/50\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 0.3915 - val_loss: 0.2613\n",
            "Epoch 26/50\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 0.4935 - val_loss: 0.3114\n",
            "Epoch 27/50\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.5952 - val_loss: 1.3701\n",
            "Epoch 28/50\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.4606 - val_loss: 0.4569\n",
            "Epoch 29/50\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 0.3805 - val_loss: 2.8083\n",
            "Epoch 30/50\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 0.3916 - val_loss: 1.2043\n",
            "Epoch 31/50\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.3956 - val_loss: 0.9095\n",
            "Epoch 32/50\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 0.4216 - val_loss: 0.7648\n",
            "Epoch 33/50\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.5021 - val_loss: 0.4733\n",
            "Epoch 34/50\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.4328 - val_loss: 1.1631\n",
            "Epoch 35/50\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.4232 - val_loss: 1.0747\n",
            "Epoch 36/50\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 0.4386 - val_loss: 1.6900\n",
            "Epoch 37/50\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 0.3888 - val_loss: 0.6305\n",
            "Epoch 38/50\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 0.3798 - val_loss: 2.0224\n",
            "Epoch 39/50\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 0.3833 - val_loss: 1.7190\n",
            "Epoch 40/50\n",
            "41/41 [==============================] - 0s 4ms/step - loss: 0.4219 - val_loss: 0.7775\n",
            "Epoch 41/50\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.4946 - val_loss: 0.9577\n",
            "Epoch 42/50\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.5129 - val_loss: 0.2523\n",
            "Epoch 43/50\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.4931 - val_loss: 0.6550\n",
            "Epoch 44/50\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.5005 - val_loss: 0.7359\n",
            "Epoch 45/50\n",
            "41/41 [==============================] - 0s 6ms/step - loss: 0.4620 - val_loss: 0.9582\n",
            "Epoch 46/50\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.6162 - val_loss: 0.3959\n",
            "Epoch 47/50\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.4832 - val_loss: 0.7394\n",
            "Epoch 48/50\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.4948 - val_loss: 1.3285\n",
            "Epoch 49/50\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 0.5594 - val_loss: 0.6422\n",
            "Epoch 50/50\n",
            "41/41 [==============================] - 0s 5ms/step - loss: 1.3064 - val_loss: 0.2591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mse 값은 작을 수록 좋다. Accuracy와 반대\n",
        "y_pred = model.predict(x_test)\n",
        "print('MSE =', mean_squared_error(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cbr5zKnAsIC",
        "outputId": "71d6edc8-1cfc-4b16-a369-9b70a27e5957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE = 0.2590523278037575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 표준화된 y_pred를 원래값으로 역변환\n",
        "y_pred = scaleY.inverse_transform(y_pred)\n",
        "# print(y_pred)"
      ],
      "metadata": {
        "id": "mNjQJip-Awij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(hist.history['loss'], label='train')\n",
        "plt.plot(hist.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "aurMaa3GAzzT",
        "outputId": "ed5528fc-c0f6-4ea3-c9b1-ed6cc07b5568"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}